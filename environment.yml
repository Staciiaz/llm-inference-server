name: llm-inference-server
channels:
  - pytorch
  - nvidia
  - defaults
dependencies:
  - pip=23.3.1
  - python=3.12.1
  - pytorch=2.2.1
  - pytorch-cuda=12.1
  - pip:
      - accelerate==0.27.2
      - annotated-types==0.6.0
      - anyio==4.3.0
      - bitsandbytes==0.42.0
      - click==8.1.7
      - colorama==0.4.6
      - distro==1.9.0
      - fastapi==0.109.2
      - fsspec==2024.2.0
      - grpcio==1.62.0
      - grpcio-tools==1.62.0
      - h11==0.14.0
      - httpcore==1.0.4
      - httpx==0.27.0
      - huggingface-hub==0.20.3
      - loguru==0.7.2
      - openai==1.12.0
      - packaging==23.2
      - protobuf==4.25.3
      - psutil==5.9.8
      - pydantic==2.6.2
      - pydantic-core==2.16.3
      - python-dotenv==1.0.1
      - regex==2023.12.25
      - safetensors==0.4.2
      - scipy==1.12.0
      - sniffio==1.3.0
      - starlette==0.36.3
      - tokenizers==0.15.2
      - torchaudio==2.2.1
      - torchvision==0.17.1
      - tqdm==4.66.2
      - transformers==4.38.1
      - uvicorn==0.27.1
      - win32-setctime==1.1.0
prefix: C:\Users\Sea\miniconda3\envs\llm-inference-server
